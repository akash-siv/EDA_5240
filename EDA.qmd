---
title: "EDA Proposal"
author: "Akash Sivasubramanian - 0862944"
format: html
---

# Project Title :
# Dataset used:
# Team Members: Akash Sivasubramanian - 0862944


Importing the required libraries
```{r}
library(tidyverse)
library(leaflet)
library(wordcloud2)
library(webshot)
webshot::install_phantomjs()
library("htmlwidgets")
library(knitr)

# Importing the dataset
tuesdata <- tidytuesdayR::tt_load('2023-10-10')
haunted_places <- tuesdata$haunted_places

# Glimpse of the dataset
haunted_places %>% glimpse()
head(haunted_places)
```
```{r}
# Counting unique values in each column
column_summary <- haunted_places %>%
  summarise(across(everything(), ~ n_distinct(.)))
column_summary
```
Wow there are 9904 unique locations in 4386 unique cities across usa. This is interesting.
Hmm things going weired because there are only 50 states in the US but we have 51 unique values in the state column. Let's dig deeper into this.

```{r}
# Get unique values in the state column in alphabetical order.
unique_states <- haunted_places %>%
  distinct(state) %>%
  arrange(state)

unique_states
```
Gottcha there is a value 'Washington D.C.' which is not a state but a federal district. Let's correct this.
we can dig deeper into the dataset to find out the haunted places in Washington D.C.

```{r}
# Haunted places in Washington D.C.
haunted_places_dc <- haunted_places %>%
  filter(state == "Washington DC")
haunted_places_dc
```
OK Ok. I think there is lot going on in the Us.
"After some web search I found that the Wasington DC is a separate entity from the US but overseen by US"
Ohh, That's there political concern. Iam going to keep it as it is.

```{r}
# Cleaning the dataset
# Removing redundant columns (Country, state_abbrev)
haunted_places <- haunted_places %>%
  select(city, description, location, state, latitude, longitude, city_latitude, city_longitude)
```

```{r}
# Function for checking missing values
missing_values <- function(df){
  
  df %>% summarise(across(everything(), ~ sum(is.na(.))))
}
missing_values(haunted_places)
```

printing the rows with missing values in city_latitude and city_longitude columns.
```{r}
# Print rows with missing values in city_latitude and city_longitude columns
missing_coordinates <- haunted_places %>%
  filter(is.na(city_latitude) | is.na(city_longitude))
missing_coordinates
```
```{r}
result <- haunted_places[haunted_places$city == "Cockeysville", ]
#result

for (city in missing_coordinates$city) {
  print(haunted_places[haunted_places$city == city, ])
}

```
by doing the city search we can see that we can find the city latidude and longitude of these cities.
Cockeysville, Faribault, Streamwood, Cynthiana
Lets fix these city coordinates. (may be later)

```{r}
# Dig deeper into the missing values
# removing the rows with missing values in city, location, city_latitude, city_longitude columns
haunted_places <- haunted_places %>%
  filter(!is.na(city) & !is.na(location) & !is.na(city_latitude) & !is.na(city_longitude))

# Checking for missing values
missing_values(haunted_places)
```

Check the Exact duplicate values in the dataset

```{r}

# Before removing the duplicates
nrow(haunted_places)

# Find exact duplicates
duplicate_rows <- haunted_places %>%
  group_by(across(everything())) %>%
  filter(n() > 1) %>%
  ungroup()

# Display the duplicate rows
print(duplicate_rows)

# Remove the Exact duplicate rows
haunted_places <- haunted_places %>%
  distinct()

# find the total number of rows in the dataset after removing the duplicates
nrow(haunted_places)

```


creating a frequency table for the city ,location and state columns
```{r}
# Frequency table for city column
city_freq <- haunted_places %>%
  count(city, sort = TRUE)

city_freq

# Frequency table for location column
location_freq <- haunted_places %>%
  count(location, sort = TRUE)

location_freq

# Frequency table for state column
state_freq <- haunted_places %>%
  count(state, sort = TRUE)

state_freq


```

Create a Heatmap for city and the state freq table.

create a Circular Packing for state - city - location


```{r}
# load wordcloud2


# wordcloud for city column
city_cloud <- wordcloud2(city_freq, size = 0.5)
city_cloud

# wordcloud for location column
location_cloud <- wordcloud2(location_freq, size = 0.5)
location_cloud

# wordcloud for state column
state_cloud <- wordcloud2(state_freq, size = 0.8)
state_cloud

```


```{r}
# Converting the the wordcloud into image

# save it in html
saveWidget(city_cloud,"tmp_city.html",selfcontained = F)
saveWidget(state_cloud,"tmp_state.html",selfcontained = F)
saveWidget(location_cloud,"tmp_location.html",selfcontained = F)

# and in png or pdf
webshot("tmp_city.html","city_cloud.png", delay =5)
webshot("tmp_state.html","state_cloud.png", delay =5)
webshot("tmp_location.html","location_cloud.png", delay =5)
```

```{r}
# Filter rows where 'location' contains "Cemetery"
cemetery_coordinates <- haunted_places %>%
  filter(grepl("cemetery", location, ignore.case = TRUE)) %>%
  select(location, latitude, longitude) %>%
  arrange(location)

school_coordinates <- haunted_places %>%
  filter(grepl("school", location, ignore.case = TRUE)) %>%
  select(location, latitude, longitude) %>%
  arrange(location)

university_coordinates <- haunted_places %>%
  filter(grepl("university", location, ignore.case = TRUE)) %>%
  select(location, latitude, longitude) %>%
  arrange(location)

# Print the result
cemetery_coordinates
school_coordinates
university_coordinates

```

```{r}
# Finding the location which have same latitude and longitude (without considering the NA values)

same_coordinates <- haunted_places %>%
  group_by(latitude, longitude) %>%
  filter(n() > 1) %>%
  ungroup()

# Remove NA values
same_coordinates <- same_coordinates %>%
  filter(!is.na(latitude) & !is.na(longitude))

same_coordinates

# Create a frequency table for the location column
same_coordinates_freq <- same_coordinates %>%
  count(location, sort = TRUE)

same_coordinates_freq
```

Create a proportion table for it.
```{r}
# Create a proportion table for the location column
same_coordinates_prop <- same_coordinates_freq %>%
  mutate(proportion = n / 870)

same_coordinates_prop

kable(same_coordinates_prop, caption = "Frequency and Proportional Table", col.names = c("Location", "Counts", "Proportions"))
```



We can see that some of the latitudes and longitudes are same for different location (but they are not, it is just the typo.)
```{r}
# function to filter the duplicate data
filter_duplicate_locations <- function(df) {
  df %>%
    group_by(latitude, longitude) %>%
    filter(n_distinct(location) > 1) %>%  # Keep groups with more than one distinct location name
    arrange(latitude, longitude)         # Arrange by latitude and longitude
}
```

```{r}
# Identify locations with the same latitude and longitude but different names
result <- filter_duplicate_locations(haunted_places)

# Remove NA values
result <- result %>%
  filter(!is.na(latitude) & !is.na(longitude))

# Print the result
result

```
Ok now we can see that some of the location names have some additional data and typos and with different cases. Let's clean this. we have total of 638 rows of the cities with more that one common lat and long.


```{r}

# Clean the location column
# Replace locations with the shortest name; if equal, use the first name
updated_location_names <- result %>%
  group_by(latitude, longitude) %>%
  mutate(location = location[which.min(nchar(location))]) %>%  # Shortest name or first in tie
  ungroup()

# Print the updated data frame
updated_location_names
```


```{r}
# Conforming it is working or not
updated_location_names <- filter_duplicate_locations(updated_location_names)

updated_location_names
```

Super. it workedðŸ˜. Now we can see that the location names are cleaned.

Now we can apply the same method for entire dataframe.

```{r}
nrow(haunted_places)
```
new claude

```{r}
# Standardize location names only for non-NA coordinates
haunted_places <- haunted_places %>%
  group_by(latitude, longitude) %>%
  mutate(
    # Only standardize when coordinates are not NA
    standardized_location = if(!any(is.na(latitude)) && !any(is.na(longitude))) {
      names(which.max(table(location)))
    } else {
      location
    }
  ) %>%
  ungroup() %>%
  mutate(location = standardized_location) %>%
  select(-standardized_location)

# Verify the results
haunted_places %>%
  group_by(latitude, longitude) %>%
  filter(n_distinct(location) > 1, 
         !is.na(latitude), 
         !is.na(longitude))
```

Yeah it worked. Now we can see that the location names are cleaned.

```{r}
# Frequency table for location column without NA values
location_freq <- haunted_places %>%
  filter(!is.na(location)) %>%
  count(location, sort = TRUE)

location_freq
```
this is the true location frequency.

these are the locations with multiple ghost siteings. lets plot it on the map.

```{r}
# Group by latitude, longitude, and location, then count occurrences
same_coordinates_freq <- haunted_places %>%
  group_by(latitude, longitude, location) %>%
  summarise(count = n(), .groups = "drop") %>%  # Count occurrences and drop grouping
  arrange(desc(count)) %>%  # Sort by count in descending order
  filter(!is.na(latitude) & !is.na(longitude))  # Remove NA values

# View the updated frequency table
same_coordinates_freq
```

get top 25 places with multiple haunted citing.
```{r}
top_50_places <- same_coordinates_freq[1:50,]
```


```{r}
# Create the leaflet map of top 50 places with multiple haunted sightings
# Show a CUSTOM circle at each position. Size defined in Pixel. Size does not change when you zoom

m=leaflet(data = top_50_places) %>%
   addTiles() %>%
   addCircleMarkers(
      ~longitude, ~latitude, 
      radius=~count*1 , 
      color=~ifelse(top_50_places$count>10 , "red", "orange"),
      stroke = TRUE, 
      fillOpacity = 0.1,
      popup = ~as.character(location)
   ) 
m
```
```{r}
# Converting the the leaflet map into image

# save it in html
saveWidget(m,"tmp_top_50.html",selfcontained = F)


# and in png or pdf
webshot("tmp_top_50.html","top_50_map.png", delay =5, vwidth = 3840,
  vheight = 2160)

```


```{r}
# save the haunted places data as a csv file
write_csv(haunted_places, "haunted_places.csv")
```

# Modelling


We can analyze if there's a relationship between distance from city center and the concentration of haunted places.
we can create a multilinear regression model to predict the number of haunted places in a city based on the distance from the city center.












