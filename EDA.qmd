---
title: "EDA Proposal"
author: "Akash Sivasubramanian - 0862944"
format: html
---

# Project Title :
# Dataset used:
# Team Members: Akash Sivasubramanian - 0862944

Importing the required libraries
```{r}
library(tidyverse)

# Importing the dataset
tuesdata <- tidytuesdayR::tt_load('2023-10-10')
haunted_places <- tuesdata$haunted_places

# Glimpse of the dataset
haunted_places %>% glimpse()
head(haunted_places)
```
```{r}
# Counting unique values in each column
column_summary <- haunted_places %>%
  summarise(across(everything(), ~ n_distinct(.)))
column_summary
```
Wow there are 9904 unique locations in 4386 unique cities across usa. This is interesting.
Hmm things going weired because there are only 50 states in the US but we have 51 unique values in the state column. Let's dig deeper into this.

```{r}
# Get unique values in the state column in alphabetical order.
unique_states <- haunted_places %>%
  distinct(state) %>%
  arrange(state)

unique_states
```
Gottcha there is a value 'Washington D.C.' which is not a state but a federal district. Let's correct this.
we can dig deeper into the dataset to find out the haunted places in Washington D.C.

```{r}
# Haunted places in Washington D.C.
haunted_places_dc <- haunted_places %>%
  filter(state == "Washington DC")
haunted_places_dc
```
OK Ok. I think there is lot going on in the Us.
"After some web search I found that the Wasington DC is a separate entity from the US but overseen by US"
Ohh, That's there political concern. Iam going to keep it as it is.

```{r}
# Cleaning the dataset
# Removing redundant columns
haunted_places <- haunted_places %>%
  select(city, description, location, state, state_abbrev, latitude, longitude, city_latitude, city_longitude)
```

```{r}
# Checking for missing values
missing_values <- haunted_places %>%
  summarise(across(everything(), ~ sum(is.na(.))))
missing_values
```
```{r}
# Dig deeper into the missing values
# removing the rows with missing values in city, location, city_latitude, city_longitude columns
haunted_places <- haunted_places %>%
  filter(!is.na(city) & !is.na(location) & !is.na(city_latitude) & !is.na(city_longitude))

# Checking for missing values
missing_values <- haunted_places %>%
  summarise(across(everything(), ~ sum(is.na(.))))
missing_values
```

Check the Exact duplicate values in the dataset

```{r}

# Before removing the duplicates
nrow(haunted_places)

# Find exact duplicates
duplicate_rows <- haunted_places %>%
  group_by(across(everything())) %>%
  filter(n() > 1) %>%
  ungroup()

# Display the duplicate rows
print(duplicate_rows)

# Remove the Exact duplicate rows
haunted_places <- haunted_places %>%
  distinct()

# find the total number of rows in the dataset after removing the duplicates
nrow(haunted_places)

```


creating a frequency table for the city ,location and state columns
```{r}
# Frequency table for city column
city_freq <- haunted_places %>%
  count(city, sort = TRUE)

city_freq

# Frequency table for location column
location_freq <- haunted_places %>%
  count(location, sort = TRUE)

location_freq

# Frequency table for state column
state_freq <- haunted_places %>%
  count(state, sort = TRUE)

state_freq


```

Create a Heatmap for city and the stae freq table.


```{r}
# load wordcloud2
library(wordcloud2) 

# wordcloud for city column
wordcloud2(city_freq, size = 0.7)

# wordcloud for location column
wordcloud2(location_freq, size = 0.7)

# wordcloud for state column
wordcloud2(state_freq, size = 0.7)

```


```{r}
# Converting the the wordcloud into image
# load wordcloud2
library(wordcloud2) 

# install webshot
library(webshot)
webshot::install_phantomjs()

# Make the graph
my_graph <- wordcloud2(demoFreq, size=1.5)

# save it in html
library("htmlwidgets")
saveWidget(my_graph,"tmp.html",selfcontained = F)

# and in png or pdf
webshot("tmp.html","fig_1.pdf", delay =5, vwidth = 480, vheight=480)
```

```{r}
# Expirementation with leaflet
library(leaflet)
# Create 20 markers (Random points)
data = data.frame(
   long=sample(seq(-150,150),20),
   lat=sample(seq(-50,50),20),
   val=round(rnorm(20),2),
   name=paste("point",letters[1:20],sep="_")
) 
 
# Show a CUSTOM circle at each position. Size defined in Pixel. Size does not change when you zoom
m=leaflet(data = data) %>%
   addTiles() %>%
   addCircleMarkers(
      ~long, ~lat, 
      radius=~val*14 , 
      color=~ifelse(data$val>0 , "red", "orange"),
      stroke = TRUE, 
      fillOpacity = 0.2,
      popup = ~as.character(name)
   ) 
m
```


```{r}
# Filter rows where 'location' contains "Cemetery"
cemetery_coordinates <- haunted_places %>%
  filter(grepl("cemetery", location, ignore.case = TRUE)) %>%
  select(location, latitude, longitude) %>%
  arrange(location)

school_coordinates <- haunted_places %>%
  filter(grepl("school", location, ignore.case = TRUE)) %>%
  select(location, latitude, longitude) %>%
  arrange(location)

university_coordinates <- haunted_places %>%
  filter(grepl("university", location, ignore.case = TRUE)) %>%
  select(location, latitude, longitude) %>%
  arrange(location)

# Print the result
cemetery_coordinates
school_coordinates
university_coordinates

```

```{r}
# Finding the location which have same latitude and longitude (without considering the NA values)

same_coordinates <- haunted_places %>%
  group_by(latitude, longitude) %>%
  filter(n() > 1) %>%
  ungroup()

# Remove NA values
same_coordinates <- same_coordinates %>%
  filter(!is.na(latitude) & !is.na(longitude))

same_coordinates

# Create a frequency table for the location column
same_coordinates_freq <- same_coordinates %>%
  count(location, sort = TRUE)

same_coordinates_freq
```

We can see that some of the latitudes and longitudes are same for different location (but they are not, it is just the typo.)

```{r}
# Identify locations with the same latitude and longitude but different names
result <- haunted_places %>%
  group_by(latitude, longitude) %>%
  filter(n_distinct(location) > 1) %>%  # Keep groups with more than one distinct location name
  arrange(latitude, longitude)         # Arrange by latitude and longitude

# Remove NA values
result <- result %>%
  filter(!is.na(latitude) & !is.na(longitude))

# Print the result
print(result)

```
Ok now we can see that some of the location names have some additional data and typos and with different cases. Let's clean this. we have total of 638 rows of the cities with more that one common lat and long.

```{r}

# Clean the location column
# Replace locations with the longest name; if equal, use the first name
updated_location_names <- result %>%
  group_by(latitude, longitude) %>%
  mutate(location = location[which.min(nchar(location))]) %>%  # Shortest name or first in tie
  ungroup()

# Print the updated data frame
updated_location_names
```


```{r}
# Conforming it is working or not
updated_location_names <- updated_location_names %>%
  group_by(latitude, longitude) %>%
  filter(n_distinct(location) > 1) %>%  # Keep groups with more than one distinct location name
  arrange(latitude, longitude)         # Arrange by latitude and longitude

updated_location_names
```

Super. it workedüòç. Now we can see that the location names are cleaned.

Now we can apply the same method for entire dataframe.

```{r}
nrow(haunted_places)
```
```{r}
# Creating a function to clean the entire df
standardize_locations <- function(df) {
  # First, group by coordinates and find places with multiple names
  standardized_df <- df %>%
    # Group by coordinates
    group_by(latitude, longitude) %>%
    # If multiple records exist for same coordinates, use the shortest name
    mutate(
      standardized_location = if(n() > 1) {
        location[which.min(nchar(location))]
      } else {
        location
      }
    ) %>%
    ungroup() %>%
    # Replace old location with standardized one
    mutate(location = standardized_location) %>%
    select(-standardized_location)  # Remove the temporary column
  
  return(standardized_df)
}

```

Applying the function.
```{r}
# Apply the standardization to your dataset
haunted_places <- standardize_locations(haunted_places)
```








check if it is working or not

```{r}
verification <- haunted_places %>%
  group_by(latitude, longitude) %>%
  filter(n_distinct(location) > 1, 
         !is.na(latitude), 
         !is.na(longitude)) %>%
  arrange(latitude, longitude)

nrow(verification)
```




```{r}
# Identify locations with the same latitude and longitude but different names
result <- haunted_places %>%
  group_by(latitude, longitude) %>%
  filter(n_distinct(location) > 1) %>%  # Keep groups with more than one distinct location name
  arrange(latitude, longitude)         # Arrange by latitude and longitude

# Remove NA values
result <- result %>%
  filter(!is.na(latitude) & !is.na(longitude))

# Print the result
print(result)
```

Yeah it worked. Now we can see that the location names are cleaned.

```{r}
# Frequency table for location column without NA values
location_freq <- haunted_places %>%
  filter(!is.na(location)) %>%
  count(location, sort = TRUE)

location_freq
```
this is the true location frequency.





Now Lets check the frequency of the location names.

```{r}
# Frequency table for location column removing NA values
location_freq <- haunted_places %>%
  filter(!is.na(location)) %>%
  count(location, sort = TRUE)

location_freq
```

count the number of unique latitude and longitude and create a frequency table for the same.
```{r}
# Count the number of unique latitude and longitude
unique_coordinates <- haunted_places %>%
  distinct(latitude, longitude)
```




